{"cells":[{"cell_type":"markdown","source":["# Walkthrough: Computing SVD via Eigen-Decomposition\n","\n","Below is a step-by-step explanation of what each part of the code is doing and why it works.\n","\n","---\n","\n","## 0) Goal and Key Idea\n","\n","We want the SVD of a real matrix $A \\in \\mathbb{R}^{m\\times n}:$\n","$$A = U\\,\\Sigma\\,V^\\top,$$\n","where:\n","- $U \\in \\mathbb{R}^{m\\times m}$ has orthonormal columns (left singular vectors),\n","- $V \\in \\mathbb{R}^{n\\times n}$ has orthonormal columns (right singular vectors),\n","- $\\Sigma$ is diagonal with non-negative singular values.\n","\n","\n","## 1) Eigen-decomposition of $A^\\top A$\n","\n","\n","- Form the symmetric, positive semidefinite matrix $A^\\top A \\in \\mathbb{R}^{n\\times n}$.\n","- Compute its eigenvalues (`eigvals`) and eigenvectors (`V`).\n","  - Columns of `V` are eigenvectors.\n","  - For exact arithmetic, all eigenvalues are $\\ge 0$. Numerically, tiny negatives can occur.\n","\n","\n","\n","## 2) Sort eigenpairs by descending eigenvalue\n","\n","- SVD convention orders singular values from largest to smallest.\n","- We reorder eigenvalues and their eigenvectors accordingly.\n","\n","\n","\n","## 3) Singular values = sqrt of eigenvalues (clipped)\n","\n","\n","- Convert eigenvalues to singular values: $\\sigma_i = \\sqrt{\\max(\\lambda_i, 0)}$.\n","- `np.clip` guards against tiny negative values due to floating-point error.\n","\n","\n","\n","## 4) Compute left singular vectors \\(U\\)\n","\n","- Using the relation $u_i = \\frac{A v_i}{\\sigma_i}$ when $\\sigma_i \\neq 0$.\n","- We skip division for effectively zero singular values (rank-deficient directions).\n","\n","\n","## 5) Orthonormalize/normalize columns of \\(U\\)\n","\n","\n","- Normalizes each nonzero column to unit length.\n","- The tiny $`1e-15`$ prevents divide-by-zero.\n","\n","> **Note:** For perfectly computed $U$, normalization would be redundant; numerically it helps keep columns unit-norm.\n","\n","\n","\n","## 7) Return $U$, $\\Sigma$, $V^\\top$\n","\n","- The function returns:\n","  - `U` (left singular vectors, as columns),\n","  - `singular_vals` (the diagonal of $\\Sigma$),\n","  - `V.T` (transpose of right singular vectors).\n","\n"],"metadata":{"id":"1G3ghc8PaKx_"},"id":"1G3ghc8PaKx_"},{"cell_type":"code","source":["# !pip install opencv-python"],"metadata":{"id":"OPOCvu8xFgb3"},"id":"OPOCvu8xFgb3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def svd_via_eig(A):\n","    \"\"\"\n","    Compute the Singular Value Decomposition (SVD) of matrix A\n","    using eigen-decomposition: A = U Œ£ V^T\n","    \"\"\"\n","    # Ensure A is float (important for numerical stability)\n","    A = np.array(A, dtype=float)\n","\n","    # ----------------------------\n","    # TODO Step 1: Compute eigen-decomposition of A^T A (utilize np.linalg.eig to get the eigenvalues and eigenvectors)\n","    # ----------------------------\n","    AtA =\n","    eigvals, V =\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    sort_idx = np.argsort(eigvals)[::-1]\n","\n","    # ----------------------------\n","    # TODO Step 2: Sort eigenvalues and eigenvectors in descending order\n","    # Hint: For the eigenvectors, we want to sort by the columns\n","    # ----------------------------\n","    eigvals =\n","    V =\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","\n","    # Step 3: Singular values are sqrt of eigenvalues (non-negative)\n","    singular_vals = np.sqrt(np.clip(eigvals, 0, None))\n","\n","    U = np.zeros((A.shape[0], V.shape[1]))\n","    for i in range(len(singular_vals)):\n","        if singular_vals[i] > 1e-12:\n","    # ----------------------------\n","    # TODO Step 4: Compute U = A V / Œ£\n","    # Avoid division by zero for zero singular values\n","    # ----------------------------\n","            U[:, i] = ( @ V[:, i]) /\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    # Step 5: Ensure orthonormality (normalize columns of U)\n","    for i in range(U.shape[1]):\n","        U[:, i] /= np.linalg.norm(U[:, i]) + 1e-15\n","\n","    # Step 6: Return U, Œ£, V^T\n","    return U, singular_vals, V.T\n","\n","# Example usage:\n","A = np.array([[3., 1., 1.],\n","              [-1., 3., 1.],\n","              [2., 2., 0.]])\n","\n","U, S, Vt = svd_via_eig(A)\n","\n","print(\"U =\\n\", U)\n","print(\"Singular values =\", S)\n","print(\"V^T =\\n\", Vt)\n","\n","# Verify reconstruction\n","A_recon = U @ np.diag(S) @ Vt\n","print(\"Reconstruction error:\", np.linalg.norm(A - A_recon))\n"],"metadata":{"id":"LD5t03Q5XVLX"},"execution_count":null,"outputs":[],"id":"LD5t03Q5XVLX"},{"cell_type":"code","source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","def load_image(image_path):\n","    \"\"\"Load image in grayscale\"\"\"\n","    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    return img\n","\n","image_path = \"face.png\"\n","img = load_image(image_path)\n","\n","# Display the original image\n","plt.imshow(img, cmap=\"gray\")\n","plt.title(\"Original Face Image (Walter White aka Heisenberg)\")\n","plt.show()"],"metadata":{"id":"zGOZ-ju9XVLX"},"execution_count":null,"outputs":[],"id":"zGOZ-ju9XVLX"},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def compress_image_svd(img, k):\n","    \"\"\"Compress an image using SVD by keeping only k singular values\"\"\"\n","    # ----------------------------\n","    # TODO Step 1: Compute full SVD using numpy's SVD\n","    # U: Left Singular Vectors. S: Singular Values. Vt: Right Singular Vectors Transposed.\n","    # Set full_matrices=False for efficiency.\n","    # ----------------------------\n","    U, S, Vt =\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    # Step 2: Keep only top-k singular values\n","    S_k = np.zeros((k, k))  # Create a k x k Sigma matrix\n","    np.fill_diagonal(S_k, S[:k])  # Fill diagonal with top-k singular values\n","    U_k = U[:, :k]  # Take first k columns of U\n","    Vt_k = Vt[:k, :]  # Take first k rows of Vt\n","\n","    # TODO Step 3: Reconstruct the image using U dot S dot V^T (use matrix multiplication function in numpy, utilize the shorthand version)\n","    compressed_img =\n","\n","    return np.clip(compressed_img, 0, 255).astype(np.uint8)  # Clip to valid pixel range\n","\n","# ----------------------------\n","# TODO Compress image using SVD utilizing compress_image_svd with k=50\n","# ----------------------------\n","k =\n","compressed_img =\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","# Display Results\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(img, cmap=\"gray\")\n","plt.title(\"Original Face Image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(compressed_img, cmap=\"gray\")\n","plt.title(f\"Compressed Face Image (k={k})\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"F3M2EnlIXVLX"},"execution_count":null,"outputs":[],"id":"F3M2EnlIXVLX"},{"cell_type":"code","source":["# ----------------------------\n","# TODO Compress image using SVD utilizing compress_image_svd with k=15\n","# ----------------------------\n","k =\n","compressed_img =\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","# Display Results\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(img, cmap=\"gray\")\n","plt.title(\"Original Face Image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(compressed_img, cmap=\"gray\")\n","plt.title(f\"Compressed Face Image (k={k})\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"MCYvnmhkYLO5"},"id":"MCYvnmhkYLO5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------------------\n","# TODO Compress image using SVD utilizing compress_image_svd with k=10\n","# ----------------------------\n","k =\n","compressed_img =\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","# Display Results\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(img, cmap=\"gray\")\n","plt.title(\"Original Face Image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(compressed_img, cmap=\"gray\")\n","plt.title(f\"Compressed Face Image (k={k})\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"KZFxNH6oYHdP"},"id":"KZFxNH6oYHdP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ----------------------------\n","# TODO Compress image using SVD utilizing compress_image_svd with k=5\n","# ----------------------------\n","k =\n","compressed_img =\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","# Display Results\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.imshow(img, cmap=\"gray\")\n","plt.title(\"Original Face Image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(compressed_img, cmap=\"gray\")\n","plt.title(f\"Compressed Face Image (k={k})\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Nth3pWUR4C_F"},"id":"Nth3pWUR4C_F","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6e74c775","metadata":{"id":"6e74c775"},"source":["\n","**Goal:** Compare **TF‚ÄìIDF ‚Üí KNN** vs **TF‚ÄìIDF ‚Üí TruncatedSVD (LSA) ‚Üí KNN** for **binary sentiment** classification on IMDB reviews.\n","\n","You will:\n","- Load IMDB reviews with text and labels.\n","- Build a TF‚ÄìIDF ‚Üí KNN baseline (sweep `k`, metric, weights).\n","- Add Truncated SVD (LSA) ‚Üí KNN and choose `k_svd` + `n_neighbors` with validation F1.\n","- Evaluate on a held-out test set and discuss trade-offs.\n","\n","> ‚úÖ **Deliverables**: code, plots, final metrics table, and a short written analysis.\n"]},{"cell_type":"markdown","id":"21da6d4a","metadata":{"id":"21da6d4a"},"source":["\n","## 0) Setup\n","\n","- Recommended: Python 3.9+  \n","- Install dependencies if needed.\n","\n","> If `datasets` is not installed, uncomment the first line below.\n"]},{"cell_type":"code","execution_count":null,"id":"519b7042","metadata":{"id":"519b7042"},"outputs":[],"source":["\n","# !pip install numpy pandas scikit-learn matplotlib datasets\n","import os, sys, math, json, random, time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import Normalizer\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n","\n","RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","random.seed(RANDOM_STATE)\n"]},{"cell_type":"markdown","id":"6a6f7592","metadata":{"id":"6a6f7592"},"source":["\n","## 1) Load IMDB & Create Splits\n"]},{"cell_type":"code","execution_count":null,"id":"b1de6c20","metadata":{"id":"b1de6c20"},"outputs":[],"source":["\n","from datasets import load_dataset\n","\n","# Load IMDB\n","ds = load_dataset(\"imdb\")\n","train_text = ds[\"train\"][\"text\"]\n","train_label = ds[\"train\"][\"label\"]\n","test_text  = ds[\"test\"][\"text\"]\n","test_label = ds[\"test\"][\"label\"]\n","\n","train_text = list(ds[\"train\"][\"text\"])\n","train_label = list(ds[\"train\"][\"label\"])\n","\n","\n","# ----------------------------\n","# TODO Create the train/validation train using the train_test_split method from sklearn where  X=train_test, y=train_label, test_size=0.2, random_state=RANDOMSTATE and stratify=train_label\n","# ----------------------------\n","X_train, X_val, y_train, y_val =\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","\n","X_test, y_test = test_text, test_label\n","print(f\"Train: {len(X_train)}  |  Val: {len(X_val)}  |  Test: {len(X_test)}\")\n"]},{"cell_type":"markdown","id":"51d25206","metadata":{"id":"51d25206"},"source":["\n","> **Fallback (optional): CSV Loader** ‚Äì If you can‚Äôt use `datasets`, put a CSV with `text` and `label` (0/1) and run this instead.\n"]},{"cell_type":"code","source":["print(np.unique(y_train))\n","# Get unique values and their counts\n","unique_values, counts = np.unique(y_train, return_counts=True)\n","\n","# Print the unique values and their counts\n","print(\"Unique values:\", unique_values)\n","print(\"Counts:\", counts)"],"metadata":{"id":"odG0vZxdFWxY"},"id":"odG0vZxdFWxY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for pos_text,text_value in enumerate(X_train[:5]):\n","  print(text_value[:120],y_train[pos_text])"],"metadata":{"id":"9E9MhdY7Os6n"},"id":"9E9MhdY7Os6n","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"eea4dcb1","metadata":{"id":"eea4dcb1"},"source":["\n","## 2) Baseline: TF‚ÄìIDF ‚Üí KNN\n"]},{"cell_type":"markdown","source":["## üìò What is TF-IDF?\n","\n","**TF-IDF** stands for **Term Frequency ‚Äì Inverse Document Frequency**.  \n","It is a numerical statistic used to reflect **how important a word is** to a document within a collection (called the *corpus*).\n","\n","Formally, it combines two ideas:\n","\n","$$\\text{TF‚ÄìIDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n","\n","where  \n","- \\(t\\) = term (word)  \n","- \\(d\\) = one document  \n","- the corpus = all documents  \n","\n","<br><br><br><br>\n","\n","**Term Frequency** measures how often a word appears in a specific document.\n","\n","$$\\text{TF}(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total number of terms in } d}$$\n","\n","TF is higher when the term occurs more frequently in the document.\n","  \n","<br><br><br><br>\n","\n","**Inverse Document Frequency** measures how *rare* or *unique* a word is across the entire corpus.\n","\n","$\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + n_t}\\right)$\n","\n","where  \n","- \\(N\\) = total number of documents  \n","- \\(n_t\\) = number of documents containing the term \\(t\\)\n","\n","Intuitively:\n","- Common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, or ‚Äúis‚Äù appear in many documents ‚Üí **low IDF**\n","- Rare or discriminative words like ‚Äúthriller‚Äù, ‚Äúblockchain‚Äù ‚Üí **high IDF**"],"metadata":{"id":"w0YRxE8k55Vh"},"id":"w0YRxE8k55Vh"},{"cell_type":"code","execution_count":null,"id":"6555c832","metadata":{"id":"6555c832"},"outputs":[],"source":["k_nn_list_vals = [2,5]\n","\n","tfidf = TfidfVectorizer(\n","    lowercase=True,\n","    stop_words=\"english\",\n","    ngram_range=(1,2),\n","    max_df=0.9,\n","    min_df=5\n",")\n","\n","def eval_knn(model, name, X_tr, y_tr, X_va, y_va):\n","    # ----------------------------\n","    # TODO use the KNN model and fit it to the X_tr using the y_tr labels\n","    # ----------------------------\n","    t0 = time.time()\n","    model\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    fit_s = time.time() - t0\n","\n","    # ----------------------------\n","    #TODO now use the model to predict on the validation set (X_va) set\n","    # ----------------------------\n","    y_hat = model\n","    pred_s = time.time() - t0 - fit_s\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    # ----------------------------\n","    #TODO calculate the accuracy_score and f1_score using sklearn.metrics\n","    # ----------------------------\n","    acc =\n","    f1  =\n","    # ----------------------------\n","    # Implementation Ends Here\n","    # ----------------------------\n","\n","    print(f\"\\n=== {name} (VAL) ===\")\n","    print(\"Accuracy:\", round(acc,4), \" F1:\", round(f1,4), f\"| fit {fit_s:.2f}s, pred {pred_s:.2f}s\")\n","    print(classification_report(y_va, y_hat, digits=4))\n","    return acc, f1\n","\n","baseline_results = []\n","for metric in [\"cosine\", \"euclidean\"]:\n","    for k in k_nn_list_vals:\n","        for w in [\"uniform\", \"distance\"]:\n","            pipe = Pipeline([\n","                (\"tfidf\", tfidf),\n","                (\"knn\", KNeighborsClassifier(n_neighbors=k, metric=metric, weights=w))\n","            ])\n","            acc, f1 = eval_knn(pipe, f\"TFIDF+KNN | k={k} | {metric} | {w}\",\n","                               X_train, y_train, X_val, y_val)\n","            baseline_results.append(dict(model=\"TFIDF+KNN\", k=k, metric=metric, weights=w, acc=acc, f1=f1))\n","\n","baseline_df = pd.DataFrame(baseline_results).sort_values([\"metric\",\"k\",\"weights\"]).reset_index(drop=True)\n","baseline_df.head()\n"]},{"cell_type":"markdown","id":"c9dd7d4e","metadata":{"id":"c9dd7d4e"},"source":["\n","## 3) Truncated SVD (LSA) ‚Üí KNN\n"]},{"cell_type":"code","execution_count":null,"id":"4dafe8c8","metadata":{"id":"4dafe8c8"},"outputs":[],"source":["\n","\n","\n","def make_svd_knn(k_components, n_neighbors, metric=\"cosine\", weights=\"distance\"):\n","    return Pipeline([\n","        (\"tfidf\", tfidf),\n","        # ----------------------------\n","        #TODO use TruncatedSVD where the n_components=k_components, random_state=RANDOM_STATE\n","        # ----------------------------\n","        (\"svd\", ),\n","        # ----------------------------\n","        # Implementation Ends Here\n","        # ----------------------------\n","        (\"norm\", Normalizer(norm=\"l2\")),\n","        (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, weights=weights))\n","    ])\n","\n","rows = []\n","k_svd_list = [50]\n","\n","for k_svd in k_svd_list:\n","    for k_nn in k_nn_list_vals:\n","        for metric in [\"cosine\", \"euclidean\"]:\n","            for w in [\"uniform\", \"distance\"]:\n","                pipe = make_svd_knn(k_svd, k_nn, metric=metric, weights=w)\n","                pipe.fit(X_train, y_train)\n","                y_val_hat = pipe.predict(X_val)\n","                acc = accuracy_score(y_val, y_val_hat)\n","                f1  = f1_score(y_val, y_val_hat)\n","                evr = pipe.named_steps[\"svd\"].explained_variance_ratio_.sum()\n","                rows.append({\"k_svd\": k_svd, \"k_nn\": k_nn, \"metric\": metric, \"weights\": w,\n","                             \"acc\": acc, \"f1\": f1, \"evr\": evr})\n","\n","svd_df = pd.DataFrame(rows).sort_values([\"k_svd\",\"k_nn\",\"metric\",\"weights\"]).reset_index(drop=True)\n","svd_df.head()\n"]},{"cell_type":"code","execution_count":null,"id":"7608a3c0","metadata":{"id":"7608a3c0"},"outputs":[],"source":["\n","# Plots (matplotlib only; single charts, default colors)\n","sub = svd_df[(svd_df[\"metric\"]==\"cosine\") & (svd_df[\"weights\"]==\"distance\")]\n","plt.figure()\n","for k_nn in sorted(sub[\"k_nn\"].unique()):\n","    s = sub[sub[\"k_nn\"]==k_nn]\n","    plt.plot(s[\"k_svd\"], s[\"f1\"], marker=\"o\", label=f\"n_neighbors={k_nn}\")\n","plt.xlabel(\"SVD components (k_svd)\"); plt.ylabel(\"Validation F1\")\n","plt.title(\"F1 vs SVD dimension (cosine, distance weights)\"); plt.grid(True); plt.legend(); plt.show()\n","\n","plt.figure()\n","for k_svd in sorted(sub[\"k_svd\"].unique()):\n","    s = sub[sub[\"k_svd\"]==k_svd]\n","    plt.plot(s[\"k_nn\"], s[\"f1\"], marker=\"o\", label=f\"k_svd={k_svd}\")\n","plt.xlabel(\"n_neighbors (k_nn)\"); plt.ylabel(\"Validation F1\")\n","plt.title(\"F1 vs neighbors (cosine, distance weights)\"); plt.grid(True); plt.legend(); plt.show()\n","\n","evr_by_k = sub.groupby(\"k_svd\")[\"evr\"].mean()\n","plt.figure()\n","plt.plot(evr_by_k.index, evr_by_k.values, marker=\"o\")\n","plt.xlabel(\"SVD components\"); plt.ylabel(\"Cumulative explained variance ratio\")\n","plt.title(\"Explained variance vs SVD dimension\"); plt.grid(True); plt.show()\n"]},{"cell_type":"markdown","id":"ec673e99","metadata":{"id":"ec673e99"},"source":["\n","## 4) Final Selection & **Test** Evaluation\n"]},{"cell_type":"code","execution_count":null,"id":"a450b5e5","metadata":{"id":"a450b5e5"},"outputs":[],"source":["\n","best = svd_df.sort_values(\"f1\", ascending=False).iloc[0]\n","best\n"]},{"cell_type":"code","execution_count":null,"id":"a3e83690","metadata":{"id":"a3e83690"},"outputs":[],"source":["X_tr_final = np.concatenate([X_train])\n","y_tr_final = np.concatenate([y_train])\n","\n","from sklearn.metrics import confusion_matrix\n","\n","final_pipe = make_svd_knn(\n","    k_components=int(best[\"k_svd\"]),\n","    n_neighbors=int(best[\"k_nn\"]),\n","    metric=str(best[\"metric\"]),\n","    weights=str(best[\"weights\"])\n",")\n","final_pipe.fit(X_tr_final, y_tr_final)\n","y_test_hat = final_pipe.predict(X_test)\n","\n","print(\"\\n=== FINAL TEST RESULTS (SVD+KNN) ===\")\n","print(\"Accuracy:\", round(accuracy_score(y_test, y_test_hat), 4))\n","print(\"F1:\", round(f1_score(y_test, y_test_hat), 4))\n","print(classification_report(y_test, y_test_hat, digits=4))\n","print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hat))\n"]},{"cell_type":"markdown","id":"de4cdd15","metadata":{"id":"de4cdd15"},"source":["\n","## 5) Compare against **TF‚ÄìIDF-only KNN**\n"]},{"cell_type":"code","execution_count":null,"id":"77ee2509","metadata":{"id":"77ee2509"},"outputs":[],"source":["\n","# Pick best TFIDF baseline from baseline_df\n","best_base = baseline_df.sort_values(\"f1\", ascending=False).iloc[0]\n","best_base\n","\n","BEST_BASE_METRIC  = best_base[\"metric\"]\n","BEST_BASE_K       = int(best_base[\"k\"])\n","BEST_BASE_WEIGHTS = best_base[\"weights\"]\n","\n","tfidf_only_best = Pipeline([\n","    (\"tfidf\", tfidf),\n","    (\"knn\", KNeighborsClassifier(n_neighbors=BEST_BASE_K, metric=BEST_BASE_METRIC, weights=BEST_BASE_WEIGHTS))\n","])\n","\n","tfidf_only_best.fit(X_tr_final, y_tr_final)\n","y_test_hat_base = tfidf_only_best.predict(X_test)\n","\n","print(f\"\\n=== FINAL TEST RESULTS (TFIDF+KNN: k={BEST_BASE_K}, {BEST_BASE_METRIC}, {BEST_BASE_WEIGHTS}) ===\")\n","print(\"Accuracy:\", round(accuracy_score(y_test, y_test_hat_base), 4))\n","print(\"F1:\", round(f1_score(y_test, y_test_hat_base), 4))\n","print(classification_report(y_test, y_test_hat_base, digits=4))\n","print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_hat_base))\n"]},{"cell_type":"markdown","id":"600fe771","metadata":{"id":"600fe771"},"source":["\n","### ‚úÖ Add this comparison table to your report\n","\n","| Features           | Model | SVD k | kNN k | Metric   | Weights   | Test Acc | Test F1 | Notes                    |\n","|--------------------|-------|------:|------:|----------|-----------|---------:|--------:|--------------------------|\n","| TF‚ÄìIDF             | KNN   |   ‚Äî   |   7   | cosine   | distance  |    ‚Ä¶     |    ‚Ä¶    | sparse, high-dim         |\n","| TF‚ÄìIDF ‚Üí **SVD**   | KNN   |  200  |   7   | cosine   | distance  |    ‚Ä¶     |    ‚Ä¶    | dense, low-rank semantic |\n"]},{"cell_type":"markdown","id":"df621150","metadata":{"id":"df621150"},"source":["\n","## 6) - Misclassification analysis\n"]},{"cell_type":"code","source":[],"metadata":{"id":"X7TfuJT5PsB8"},"id":"X7TfuJT5PsB8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"87b6afba","metadata":{"id":"87b6afba"},"outputs":[],"source":["\n","# Show a few misclassified examples for error analysis\n","\n","def show_examples(X_raw, y_true, y_pred, want=5, which=\"fp\"):\n","    idxs = []\n","    all_correct_tp = []\n","    all_correct_tn = []\n","    for i, (yt, yp) in enumerate(zip(y_true, y_pred)):\n","        if which == \"fp\" and yt == 0 and yp == 1:\n","            idxs.append(i)\n","        if which == \"fn\" and yt == 1 and yp == 0:\n","            idxs.append(i)\n","        if yt == 1 and yp == 1:\n","          all_correct_tp.append(i)\n","        elif yt == 0 and yp == 0:\n","          all_correct_tn.append(i)\n","    idxs = idxs[:want]\n","    print(len(all_correct_tp))\n","    print(len(all_correct_tn))\n","    print(f\"\\nShowing {which.upper()} ({len(idxs)}):\")\n","    for i in idxs:\n","        print(f\"\\n--- idx={i}  true={y_true[i]}  pred={y_pred[i]} ---\")\n","        #print(X_raw[i][:500].replace(\\\"\\\\n\\\",\\\" \\\"))\n","        print(X_raw[i][:500])\n","\n","\n","# ----------------------------\n","# TODO: Use the SVD + KNN model we defined and predict on our validation set\n","#       show_examples will take the validation set with the corresponding X_val and y_val\n","#       along with your predictions\n","# ----------------------------\n","y_val_hat_best =\n","show_examples(want=1, which=\"fp\")\n","show_examples(want=1, which=\"fn\")\n","# ----------------------------\n","# Implementation Ends Here\n","# ----------------------------\n","\n","\n","\n"]},{"cell_type":"code","source":["print(y_val_hat_best[:10])\n","print(y_val[:10])\n","print(len(y_val))"],"metadata":{"id":"75sJcTtVPxZB"},"id":"75sJcTtVPxZB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The next part of the lab will involve submitting to a Kaggle competition in prepartion for the mid-term\n","\n","You can find the Kaggle competition at https://www.kaggle.com/competitions/cs-506-practice-compeition"],"metadata":{"id":"VdeJFuIxSt_H"},"id":"VdeJFuIxSt_H"},{"cell_type":"markdown","source":["1) First you will download the test_df.csv from the Kaggle compeition\n","2) You will use your best trained model to perform predictions on the test_df.csv\n","3) Submit the my_predictions.csv to the Kaggle competition\n","4) The train_df.csv on the Kaggle compeition is the same as the training data we used in this lab"],"metadata":{"id":"P2RvBjgdTWBU"},"id":"P2RvBjgdTWBU"},{"cell_type":"code","source":["text_values = []\n","numbers = []\n","with open('test_df.csv') as f:\n","  for pos_line,line in enumerate(f):\n","    if pos_line == 0:\n","      continue\n","    text_ = line.strip().split(',', 1)[1]\n","    nums = line.strip().split(',', 1)[0]\n","    #print(nums)\n","    text_values.append(text_)\n","    numbers.append(nums)\n","    #if pos_line == 1000:\n","    #  break"],"metadata":{"id":"gk7KTnbNTelN"},"id":"gk7KTnbNTelN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_hat = final_pipe.predict(text_values)"],"metadata":{"id":"hZdGMlNPThI4"},"id":"hZdGMlNPThI4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = {'id': numbers,'label': y_test_hat}\n","df = pd.DataFrame(data)\n","df.to_csv('my_prediction.csv',index=False)"],"metadata":{"id":"8U1n-m8LTjJi"},"id":"8U1n-m8LTjJi","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}